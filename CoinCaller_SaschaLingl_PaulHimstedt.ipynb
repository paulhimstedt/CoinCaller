{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoinCaller_SaschaLingl_PaulHimstedt.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ybVkr0SjOagC",
        "YJFhaLzgPDnk",
        "Dpb2i9KhPibN",
        "5WVHvyDTP6N8",
        "OBdwN9idQpQU",
        "9jVykBYpQ44Q",
        "OB0KxomvRyKg",
        "HLlUbd-MSNWo"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulhimstedt/CoinCaller/blob/master/CoinCaller_SaschaLingl_PaulHimstedt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBau98k_M73-"
      },
      "source": [
        "#**Studienleistung 2 IL_2 Sascha Lingl und Paul Himstedt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMAqGuWXNYSb"
      },
      "source": [
        "## Coin Caller"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUJs_XpjNT3M"
      },
      "source": [
        "### Formulierung der Frage bezüglich aktuellen Geschehnissen am Krypto Markt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "JRJZ3S1iM2yQ"
      },
      "source": [
        "question_input_string = input(\"Your Crypto-Question: \")\n",
        "suggestions = extractKeyword(question_input_string);\n",
        "\n",
        "keyword = input(\"Please enter keyword or hashtag to perform Sentiment Analysis in: (Recommended: \"+suggestions+\" ) \")\n",
        "noOfTweet = int(input (\"Please enter how many tweets to analyze regarding Sentiment: (Recommended: 500 ) \"))\n",
        "\n",
        "print(question_input_string,keyword,noOfTweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybVkr0SjOagC"
      },
      "source": [
        "### Hier werden die nötigen Installationen und Imports vorgenommen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uREypT9_vfm4"
      },
      "source": [
        "#First Installation can take up to 5 min\n",
        "# Install the latest release of Haystack in your own environment \n",
        "! pip install farm-haystack\n",
        "\n",
        "# Install the latest master of Haystack\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git\n",
        "!pip install urllib3==1.25.4\n",
        "\n",
        "#imports\n",
        "import re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import tweepy as tw\n",
        "import os\n",
        "import datetime\n",
        "import itertools\n",
        "from datetime import date\n",
        "#keys\n",
        "consumer_key= \"TMgCLhdfwOw5TcdqWyPNDqTAs\"\n",
        "consumer_secret= \"ATYzvZerlvSd2SsInf9KLtgXAFILm3gXY6Vocj0S77MVM0VvBE\"\n",
        "access_token= \"3297798173-zU5fbHhbYWAmGTCCX5loJITmgK19SHkrKxN9aF1\"\n",
        "access_token_secret= \"xMYHAIe9D9YZGo8HaRHmIIoNeQg9QHmkENjUIeovdWKVS\"\n",
        "\n",
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "# This requires Python’s OS module to create directory\n",
        "import os.path\n",
        "\n",
        "# In Colab / No Docker environments: Start Elasticsearch from source\n",
        "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "! chown -R daemon:daemon elasticsearch-7.9.2\n",
        "\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n",
        "                   stdout=PIPE, stderr=STDOUT,\n",
        "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
        "                  )\n",
        "# wait until ES has started\n",
        "! sleep 30\n",
        "\n",
        "# Connect to Elasticsearch\n",
        "\n",
        "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
        "crypto_tweets_document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"crypro_tweet_document_store\")\n",
        "\n",
        "#now we preprocess the textual data\n",
        "\n",
        "from haystack.reader.farm import FARMReader\n",
        "from haystack.utils import print_answers\n",
        "from haystack.file_converter import TextConverter\n",
        "from haystack.preprocessor.utils import convert_files_to_dicts\n",
        "from haystack.preprocessor.preprocessor import PreProcessor\n",
        "from haystack.retriever.sparse import ElasticsearchRetriever\n",
        "from haystack.pipeline import ExtractiveQAPipeline\n",
        "\n",
        "#Sentiment Search Word prediction Imports\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from operator import itemgetter\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "#imports for Sentiment Analysis\n",
        "import tweepy as tweepy\n",
        "from textblob import TextBlob, Word, Blobber\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from textblob.taggers import NLTKTagger\n",
        "import nltk.sentiment\n",
        "import nltk.sentiment.vader\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "#Creating PieCart\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJFhaLzgPDnk"
      },
      "source": [
        "### Hier wird festgelegt wohin die gesammelten Daten gespeichert werden, Außerdem kann man hier den Umfang der Suche mit den zu durchsuchenden Hastags festlegen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7LhVy_obR_0"
      },
      "source": [
        "# 'mkdir' creates a directory in current directory.\n",
        "if(os.path.isdir(\"/content/tweet_data/\") == False):\n",
        "  os.mkdir('tweet_data')\n",
        "\n",
        "#define hashtags to search in\n",
        "altcoins_hashtags_string = [\"#bitcoin\",\"#ethereum\",\"#Cardano\",\"#crypto\",\"cryptocurrency\",\"#BinanceCoin\",\n",
        "                          \"#Polkadot\",\"#ChainLink\",\"#Solana\",\"#Litecoin\",\"#Polygon\"]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpb2i9KhPibN"
      },
      "source": [
        "### Nun werden basierend auf den Hastags Daten über die Twitter API gefetched"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd62TfMF8ffg"
      },
      "source": [
        "#took 16 min for 51 hashtags\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "date_since = date.today()\n",
        "print(date_since)\n",
        "#inspired from https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-a-list-of-lists\n",
        "\n",
        "all_user_locs = []\n",
        "for entries in temp:#cryto_hashtags_string:\n",
        "  entries+=\"-filter:verified -filter:links -filter:replies\";\n",
        "  tweets = tw.Cursor(api.search,\n",
        "              q=entries, \n",
        "              lang=\"en\",\n",
        "              since=date_since).items(1000)\n",
        "\n",
        "  all_user_locs.append([[tweet.user.screen_name,tweet.user.created_at, tweet.text, tweet.favorite_count, tweet.entities[\"hashtags\"]] for tweet in tweets])\n",
        "users_locs = flatten(all_user_locs)\n",
        "\n",
        "print(len(all_user_locs))\n",
        "print(len(users_locs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WVHvyDTP6N8"
      },
      "source": [
        "### Hier werden einige Preprocessing Schritte vorgenommen um die manchmal unübersichtlichen Tweets zu säubern. Diese werden dann auch anschließend gespeichert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThZpfH3O8ucf"
      },
      "source": [
        "tweet_text = pd.DataFrame(data=users_locs, \n",
        "                    columns=[\"name\",\"date\", \"content\", \"likes\", \"hashtags\"])\n",
        "def clean_data():\n",
        "\n",
        "  #delete links\n",
        "  tweet_text[\"content\"].replace(\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\", \"\", regex = True, inplace = True)\n",
        "                      \n",
        "  #delete RT Tags\n",
        "  tweet_text[\"content\"].replace(\"RT\", \"\", regex = True, inplace = True)\n",
        "\n",
        "  print(len(tweet_text['content'].tolist()))\n",
        "  #remove duplicates\n",
        "  tweet_text.drop_duplicates(subset='content',keep='first', inplace=True)\n",
        "  print(len(tweet_text['content'].tolist()))  \n",
        "\n",
        "  #delete formatting\n",
        "  #Quelle: https://gist.github.com/smram/d6ded3c9028272360eb65bcab564a18a\n",
        "  tweet_text[\"content\"].replace(to_replace=[r\"\\t|\\n|\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
        "\n",
        "  #delete @names\n",
        "  tweet_text[\"content\"].replace(\"@([^\\s]*)\", \"\", regex = True, inplace = True)\n",
        "\n",
        "  #delete hashtags\n",
        "  #tweet_text[\"content\"].replace(\"#([^\\s]*)\", \"\", regex = True, inplace = True)\n",
        "\n",
        "  #delete leading whitespaces\n",
        "  tweet_text[\"content\"].replace(r\"^ +| +$\", r\"\", regex=True, inplace=True)\n",
        "\n",
        "#adds a dot to the end of eachindividual tweet to make it readable as a sentence\n",
        "def make_tweets_to_sentences(list):\n",
        "  for tweets in list:\n",
        "    tweets = tweets+\".\"\n",
        "    print(tweets)\n",
        "\n",
        "clean_data()\n",
        "\n",
        "content_list = tweet_text['content'].tolist()\n",
        "\n",
        "# punkte werden hinzugefügt\n",
        "make_tweets_to_sentences(content_list)\n",
        "content_list\n",
        "\n",
        "if((os.path.isfile(\"/content/tweet_data/tweet_data0.txt\") == False)):\n",
        "  print(\"doesnt exit\")\n",
        "  with open(\"tweet_data/tweet_data0.txt\", \"w\") as output:\n",
        "      output.write(str(content_list))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBdwN9idQpQU"
      },
      "source": [
        "### Hier erstellen wir eine Datei, welche alle bereinigten Tweets umfasst, um das QA-System aufzubauen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBwyUQHOL1ix"
      },
      "source": [
        "#Now we created a file that contains all the saved tweets, we will build a QA system using Haystack in the following\n",
        "all_collected_tweets = convert_files_to_dicts(dir_path=\"tweet_data/\")\n",
        "\n",
        "crypto_tweets_preprocessor = PreProcessor(\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=False,\n",
        "    split_by=\"sentence\", # Dokument wird anhand von Sätzen getrennt\n",
        "    split_length=10, # Nach 10 Sätzen erfolgt die Trennung -> ein Dokument besteht aus 10 Tweets\n",
        "    split_overlap = 2, # 2 Sätze Überlappung bei den einzelnen Dokumenten\n",
        "    split_respect_sentence_boundary=False\n",
        ")\n",
        "nested_docs = [crypto_tweets_preprocessor.process(doc) for doc in all_collected_tweets]\n",
        "crypto_tweet_docs = [doc for x in nested_docs for doc in x]\n",
        "\n",
        "print(f\"n_files_input: {len(all_collected_tweets)}\\nn_docs_output: {len(crypto_tweet_docs)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jVykBYpQ44Q"
      },
      "source": [
        "### Jetzt konfigurieren wir den Reader und Retriever unserer Suchmaschiene. Außerdem können wir hier festlegen welches Language Modell verwendet werden soll."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnAy615tMvaQ"
      },
      "source": [
        "#Now we configure the reader and retriever\n",
        "# \"ahotrod/albert_xxlargev1_squad2_512\"                     bis zu 22min\n",
        "# \"deepset/roberta-base-squad2\"                             bis zu 2min\n",
        "# \"deepset/bert-large-uncased-whole-word-masking-squad2\"    bis zu 5min\n",
        "# \"deepset/minilm-uncased-squad2\"                           unter 1 min\n",
        "\n",
        "model_name_or_path = \"deepset/roberta-base-squad2\"\n",
        "crypto_tweets_document_store.write_documents(crypto_tweet_docs)\n",
        "crypto_tweet_retriever = ElasticsearchRetriever(document_store=crypto_tweets_document_store)\n",
        "crypto_tweets_reader = FARMReader(model_name_or_path, use_gpu=True,top_k=10)\n",
        "crypto_pipe = ExtractiveQAPipeline(crypto_tweets_reader, crypto_tweet_retriever)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qlb5_DRaRm4m"
      },
      "source": [
        "###Hier werden die Ergebnisse der Suchmaschine angezeigt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGlzR4XePR5p"
      },
      "source": [
        "crypto_prediction = crypto_pipe.run(query=question_input_string)\n",
        "print_answers(crypto_prediction, details=\"minimal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB0KxomvRyKg"
      },
      "source": [
        "### Hier werden aus der Suchanfrage mögliche Keywords extrahiert um einen Vorschag für die anstehende Sentiment Analyse zu generieren"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as5UFFJs3iI2"
      },
      "source": [
        "def extractKeyword(input):\n",
        "  arr = input.split()\n",
        "  keyword_filter =[\"Bitcoin\", \"Ethereum\",\"Cardano\", \"Crypto\", \"Cryptocurrency\", \"BinanceCoin\", \"Polkadot\", \"ChainLink\", \"Solana\", \"Litecoin\", \"Polygon\", \"Tether\", \"Monero\", \"Stellar\", \"Pancakeswap\"];\n",
        "  for x in arr:\n",
        "    for y in keyword_filter:\n",
        "      if(x == y):\n",
        "        return y;\n",
        "  return input;"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLlUbd-MSNWo"
      },
      "source": [
        "### Hier wird am Ende noch ein Sentiment Diagramm zu dem eingegebenen Suchbegriff erzeugt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5B7qlYFA2D_"
      },
      "source": [
        "#https://towardsdatascience.com/step-by-step-twitter-sentiment-analysis-in-python-d6f650ade58d\n",
        "def percentage(part,whole):\n",
        "    return 100 * float(part)/float(whole) \n",
        "\n",
        "tweets = tweepy.Cursor(api.search, q=keyword).items(noOfTweet)\n",
        "positive  = 0\n",
        "negative = 0\n",
        "neutral = 0\n",
        "polarity = 0\n",
        "tweet_list = []\n",
        "neutral_list = []\n",
        "negative_list = []\n",
        "positive_list = []\n",
        "\n",
        "for tweet in tweets:\n",
        "    \n",
        "    tweet_list.append(tweet.text)\n",
        "    analysis = TextBlob(tweet.text)\n",
        "    score = sid.polarity_scores(tweet.text)\n",
        "    neg = score['neg']\n",
        "    neu = score['neu']\n",
        "    pos = score['pos']\n",
        "    comp = score['compound']\n",
        "    polarity += analysis.sentiment.polarity\n",
        "  \n",
        "    if neg > pos:\n",
        "        negative_list.append(tweet.text)\n",
        "        negative += 1\n",
        "\n",
        "    elif pos > neg:\n",
        "        positive_list.append(tweet.text)\n",
        "        positive += 1\n",
        "    \n",
        "    elif pos == neg:\n",
        "        neutral_list.append(tweet.text)\n",
        "        neutral += 1\n",
        "\n",
        "positive = percentage(positive, noOfTweet)\n",
        "negative = percentage(negative, noOfTweet)\n",
        "neutral = percentage(neutral, noOfTweet)\n",
        "polarity = percentage(polarity, noOfTweet)\n",
        "positive = format(positive, '.1f')\n",
        "negative = format(negative, '.1f')\n",
        "neutral = format(neutral, '.1f')\n",
        "\n",
        "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
        "tweet_list = pd.DataFrame(tweet_list)\n",
        "neutral_list = pd.DataFrame(neutral_list)\n",
        "negative_list = pd.DataFrame(negative_list)\n",
        "positive_list = pd.DataFrame(positive_list)\n",
        "print(\"total number: \",len(tweet_list))\n",
        "print(\"positive number: \",len(positive_list))\n",
        "print(\"negative number: \", len(negative_list))\n",
        "print(\"neutral number: \",len(neutral_list))\n",
        "\n",
        "labels = [\"Positive [\"+positive+\"%]\" , \"Neutral [\"+neutral+\"%]\",\"Negative [\"+negative+\"%]\"]\n",
        "sizes = [positive, neutral, negative]\n",
        "colors = [\"yellowgreen\", \"blue\",\"red\"]\n",
        "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
        "plt.style.use(\"default\")\n",
        "plt.legend(labels)\n",
        "plt.title(\"Stimmungsbild= \"+keyword+\"\" )\n",
        "plt.axis(\"equal\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}